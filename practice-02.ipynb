{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4196185286103542"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "corpus = 'F:/AI-STU/1.1Syntax_Tree and AI-model/assignment-01/movie_comments.txt'\n",
    "#FILE = open(corpus).read()\n",
    "FILE = open(corpus, 'r', encoding = 'UTF-8').read()\n",
    "\n",
    "def generate_by_pro(text_corpus, length = 20):\n",
    "    return ''.join(random.sample(text_corpus, length))#从序列text_corpus中随机抽取20个元素，并将20个元素以list的形式返回\n",
    "\n",
    "generate_by_pro(corpus)\n",
    "len(FILE)#FILE总长为715496\n",
    "max_length = 100000\n",
    "sub_file = FILE[:max_length]\n",
    "\n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))#jieba.cut()用于分词，主要有全模式（显示所有出现词典中的词语），\n",
    "\n",
    "#精确模式（每个字只组成一种形式，一般默认也为该模式），搜索引擎模式（cut_for_search）\n",
    "TOKENS = cut(sub_file)\n",
    "len(TOKENS)#TOKENS总长为51046，即通过jieba.cut()后100000字的文档被分为51046part，即生成的list长度为51046\n",
    "words_count = Counter(TOKENS)#Counter()函数返回的序列中，依照计数重复元素相同次数，元素顺序是无序的\n",
    "words_count.most_common()#返回前20名的元素及其出现次数，是一个dict\n",
    "words_with_fre = [f for w, f in words_count.most_common()]#返回(w,f)中的f；即元素出现次数的排序\n",
    "words_with_fre[:10]#取排名前10的次数，返回是list形式\n",
    "#plt.plot(np.log(np.log(words_with_fre)))\n",
    "\n",
    "_2_gram_words = [\n",
    "    TOKENS[i] + TOKENS[i+1] for i in range(len(TOKENS)-1)\n",
    "]# wiwi+1，这两个词组合在一起\n",
    "_2_gram_word_counts = Counter(_2_gram_words)#wiwi+1同时出现的次数\n",
    "words_count.most_common()[-1][-1]\n",
    "\n",
    "def get_1_gram_count(word):\n",
    "    if word in words_count: return words_count[word]\n",
    "    else:\n",
    "        return words_count.most_common()[-1][-1]#如果没有该词，返回1\n",
    "    \n",
    "\n",
    "def get_2_gram_count(word):\n",
    "    if word in _2_gram_word_counts:return _2_gram_word_counts[word]\n",
    "    else:\n",
    "        return _2_gram_word_counts.most_common()[-1][-1]\n",
    "\n",
    "def get_gram_count(word, wc):\n",
    "    if word in wc: return wc[word]\n",
    "    else:\n",
    "        return wc.most_common()[-1][-1]\n",
    "    \n",
    "get_gram_count('战狼', words_count)#367\n",
    "get_gram_count('战狼2', _2_gram_word_counts)#154\n",
    "\n",
    "def two_gram_model(sentence):\n",
    "    #2-gram language model\n",
    "    tokens = cut(sentence)\n",
    "    \n",
    "    probability = 1\n",
    "    \n",
    "    for i in range(len(tokens)-1):\n",
    "        word = tokens[i]\n",
    "        next_word = tokens[i+1]\n",
    "        \n",
    "        \n",
    "        _two_gram_c = get_gram_count(word + next_word, _2_gram_word_counts)\n",
    "        _one_gram_c = get_gram_count(word, words_count)\n",
    "        pro = _two_gram_c / _one_gram_c\n",
    "        \n",
    "        probability *= pro\n",
    "        \n",
    "    return probability\n",
    "two_gram_model('战狼2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_iw_{i+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "seg_list = cut(\"我来到北京清华大学\")\n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))\n",
    "len(seg_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
